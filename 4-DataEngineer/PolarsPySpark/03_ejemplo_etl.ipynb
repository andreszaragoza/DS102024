{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejemplo ETL con Polars: Dataset de Taxis de Nueva York\n",
    "\n",
    "En esta sección, implementaremos un ejemplo completo de ETL (Extracción, Transformación y Carga) utilizando Polars para procesar el dataset de taxis de Nueva York. Este ejemplo demostrará las ventajas de Polars sobre Pandas en términos de rendimiento y funcionalidades.\n",
    "\n",
    "Nuestro ETL incluirá:\n",
    "1. Extracción de datos desde archivos Parquet\n",
    "2. Transformación y limpieza de datos con Polars\n",
    "3. Validación de datos con Pydantic\n",
    "4. Carga de datos en una base de datos SQLite utilizando SQLAlchemy\n",
    "5. Implementación de DAGs (Directed Acyclic Graphs)\n",
    "6. Configuración de logging para seguimiento del proceso\n",
    "\n",
    "Comencemos explorando la estructura del proyecto y los componentes principales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estructura del Proyecto\n",
    "\n",
    "Nuestro proyecto ETL está organizado de la siguiente manera:\n",
    "\n",
    "```\n",
    "notebook_polars_pyspark/\n",
    "├── data/\n",
    "│   └── yellow_tripdata.parquet  # Dataset de taxis de Nueva York\n",
    "├── etl_example/\n",
    "│   ├── __init__.py\n",
    "│   ├── etl_config.py      # Configuración del ETL\n",
    "│   ├── models.py          # Modelos Pydantic para validación\n",
    "│   ├── database.py        # Configuración de SQLAlchemy\n",
    "│   ├── logger.py          # Configuración de logging\n",
    "│   ├── etl_dag.py         # Implementación de DAGs\n",
    "│   ├── output/            # Directorio para la base de datos\n",
    "│   └── logs/              # Directorio para logs\n",
    "└── notebooks/\n",
    "```\n",
    "\n",
    "Vamos a examinar cada componente del ETL en detalle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuración del ETL (etl_config.py)\n",
    "\n",
    "El archivo `etl_config.py` contiene la configuración básica para nuestro ETL, incluyendo rutas de archivos, configuración de la base de datos y parámetros de logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\"\"\n",
      "Configuración para el ETL de taxis de Nueva York\n",
      "\"\"\"\n",
      "import os\n",
      "from pathlib import Path\n",
      "\n",
      "# Rutas de archivos\n",
      "BASE_DIR = Path(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n",
      "DATA_DIR = BASE_DIR / \"data\" / \"processed\"\n",
      "OUTPUT_DIR = BASE_DIR / \"etl_example\" / \"output\"\n",
      "LOG_DIR = BASE_DIR / \"etl_example\" / \"logs\"\n",
      "\n",
      "# Asegurar que los directorios existan\n",
      "OUTPUT_DIR.mkdir(exist_ok=True)\n",
      "LOG_DIR.mkdir(exist_ok=True)\n",
      "\n",
      "# Configuración de la base de datos\n",
      "DB_PATH = OUTPUT_DIR / \"nyc_taxi.db\"\n",
      "DB_URI = f\"sqlite:///{DB_PATH}\"\n",
      "\n",
      "# Configuración de logging\n",
      "LOG_FILE = LOG_DIR / \"etl_process.log\"\n",
      "LOG_LEVEL = \"INFO\"\n",
      "LOG_FORMAT = \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n",
      "\n",
      "# Configuración del dataset\n",
      "TAXI_DATA_FILE = DATA_DIR / \"yellow_tripdata.parquet\"\n",
      "TAXI_DATA_FILE_SMALL = DATA_DIR / \"yellow_tripdata_small.parquet\"\n",
      "\n",
      "# Configuración de procesamiento\n",
      "BATCH_SIZE = 100000  # Número de filas a procesar en cada lote\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Mostrar el contenido del archivo etl_config.py\n",
    "from pathlib import Path\n",
    "\n",
    "contenido = Path(\"etl_example/etl_config.py\").read_text(encoding=\"utf-8\")\n",
    "print(contenido)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Modelos de Datos con Pydantic (models.py)\n",
    "\n",
    "Utilizamos Pydantic para definir modelos de datos con validación estricta de tipos. Esto nos permite asegurar que los datos cumplen con nuestras expectativas antes de cargarlos en la base de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar el contenido del archivo models.py\n",
    "contenido = Path(\"etl_example/models.py\").read_text(encoding=\"utf-8\")\n",
    "print(contenido)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ventajas de Pydantic para Validación de Datos\n",
    "\n",
    "Pydantic ofrece varias ventajas para la validación de datos en flujos ETL:\n",
    "\n",
    "1. **Validación de tipos en tiempo de ejecución**: Pydantic valida automáticamente los tipos de datos y convierte valores cuando es posible.\n",
    "2. **Validadores personalizados**: Podemos definir funciones de validación personalizadas para reglas de negocio específicas.\n",
    "3. **Documentación integrada**: Los modelos Pydantic son autodocumentados con descripciones de campos.\n",
    "4. **Integración con FastAPI y otras bibliotecas**: Pydantic se integra bien con el ecosistema de Python.\n",
    "5. **Manejo de errores detallado**: Proporciona mensajes de error claros cuando la validación falla."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuración de la Base de Datos con SQLAlchemy (database.py)\n",
    "\n",
    "Utilizamos SQLAlchemy para definir el esquema de la base de datos y gestionar las conexiones. SQLAlchemy nos permite trabajar con bases de datos de manera orientada a objetos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar el contenido del archivo database.py\n",
    "contenido = Path(\"etl_example/database.py\").read_text(encoding=\"utf-8\")\n",
    "print(contenido)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ventajas de SQLAlchemy para ETL\n",
    "\n",
    "SQLAlchemy ofrece varias ventajas para los procesos ETL:\n",
    "\n",
    "1. **Abstracción de la base de datos**: Podemos cambiar el motor de base de datos sin modificar el código.\n",
    "2. **Mapeo objeto-relacional (ORM)**: Trabajamos con objetos Python en lugar de SQL directo.\n",
    "3. **Gestión de sesiones**: Manejo eficiente de transacciones y conexiones.\n",
    "4. **Migraciones de esquema**: Facilita la evolución del esquema de la base de datos.\n",
    "5. **Validación a nivel de base de datos**: Complementa la validación de Pydantic con restricciones a nivel de base de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Configuración de Logging (logger.py)\n",
    "\n",
    "El sistema de logging nos permite seguir el progreso del ETL y diagnosticar problemas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar el contenido del archivo logger.py\n",
    "contenido = Path(\"etl_example/logger.py\").read_text(encoding=\"utf-8\")\n",
    "print(contenido)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Implementación de DAGs (etl_dag.py)\n",
    "\n",
    "Para implementar DAGs (Directed Acyclic Graphs) que definen el flujo de trabajo del ETL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar el contenido del archivo etl_dag.py\n",
    "contenido = Path(\"etl_example/etl_dag.py\").read_text(encoding=\"utf-8\")\n",
    "print(contenido)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejecutando el ETL\n",
    "\n",
    "Ahora vamos a ejecutar nuestro ETL y analizar su rendimiento. Primero, importamos los módulos necesarios y configuramos el entorno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuración completada. La base de datos se creará en: c:\\Users\\anoni\\Documents\\GitHub\\DS102024_2parte\\4-DataEngineer\\PolarsPySpark\\etl_example\\output\\nyc_taxi.db\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# Añadir el directorio raíz al path para poder importar los módulos\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "# Importar los módulos del ETL\n",
    "from etl_example.etl_dag import nyc_taxi_etl_flow\n",
    "from etl_example.etl_config import DB_PATH, OUTPUT_DIR\n",
    "\n",
    "# Asegurar que el directorio de salida existe\n",
    "OUTPUT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Eliminar la base de datos si existe para empezar desde cero\n",
    "if DB_PATH.exists():\n",
    "    DB_PATH.unlink()\n",
    "\n",
    "print(f\"Configuración completada. La base de datos se creará en: {DB_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora ejecutamos el flujo ETL y medimos el tiempo que tarda en completarse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:42:02,120 - nyc_taxi_etl - INFO - Iniciando flujo ETL para datos de taxis de Nueva York\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:42:02,120 [INFO] Iniciando flujo ETL para datos de taxis de Nueva York\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:42:02,124 - nyc_taxi_etl - INFO - Extrayendo datos del archivo: c:\\Users\\anoni\\Documents\\GitHub\\DS102024_2parte\\4-DataEngineer\\PolarsPySpark\\data\\processed\\yellow_tripdata_small.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:42:02,124 [INFO] Extrayendo datos del archivo: c:\\Users\\anoni\\Documents\\GitHub\\DS102024_2parte\\4-DataEngineer\\PolarsPySpark\\data\\processed\\yellow_tripdata_small.parquet\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:42:14,231 - nyc_taxi_etl - INFO - Datos extraídos exitosamente. Filas: 2964624, Columnas: 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:42:14,231 [INFO] Datos extraídos exitosamente. Filas: 2964624, Columnas: 19\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:42:14,234 - nyc_taxi_etl - INFO - Iniciando transformación de datos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:42:14,234 [INFO] Iniciando transformación de datos\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:42:19,115 - nyc_taxi_etl - INFO - Transformación completada. Filas restantes: 2870076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:42:19,115 [INFO] Transformación completada. Filas restantes: 2870076\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:42:19,118 - nyc_taxi_etl - INFO - Iniciando validación de datos con Pydantic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:42:19,118 [INFO] Iniciando validación de datos con Pydantic\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:42:19,841 - nyc_taxi_etl - WARNING - Error de validación en el registro 6789: 1 validation error for TaxiTrip\n",
      "total_amount\n",
      "  Value error, Los montos deben ser positivos o cero [type=value_error, input_value=-4.0, input_type=float]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/value_error\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:42:19,841 [WARNING] Error de validación en el registro 6789: 1 validation error for TaxiTrip\n",
      "total_amount\n",
      "  Value error, Los montos deben ser positivos o cero [type=value_error, input_value=-4.0, input_type=float]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/value_error\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:42:19,982 - nyc_taxi_etl - WARNING - Error de validación en el registro 15250: 1 validation error for TaxiTrip\n",
      "total_amount\n",
      "  Value error, Los montos deben ser positivos o cero [type=value_error, input_value=-4.0, input_type=float]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/value_error\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:42:19,982 [WARNING] Error de validación en el registro 15250: 1 validation error for TaxiTrip\n",
      "total_amount\n",
      "  Value error, Los montos deben ser positivos o cero [type=value_error, input_value=-4.0, input_type=float]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/value_error\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:42:20,878 - nyc_taxi_etl - WARNING - Error de validación en el registro 74129: 1 validation error for TaxiTrip\n",
      "total_amount\n",
      "  Value error, Los montos deben ser positivos o cero [type=value_error, input_value=-5.75, input_type=float]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/value_error\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:42:20,878 [WARNING] Error de validación en el registro 74129: 1 validation error for TaxiTrip\n",
      "total_amount\n",
      "  Value error, Los montos deben ser positivos o cero [type=value_error, input_value=-5.75, input_type=float]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/value_error\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:42:22,170 - nyc_taxi_etl - WARNING - Error de validación en el registro 140013: 1 validation error for TaxiTrip\n",
      "total_amount\n",
      "  Value error, Los montos deben ser positivos o cero [type=value_error, input_value=-3.25, input_type=float]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/value_error\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:42:22,170 [WARNING] Error de validación en el registro 140013: 1 validation error for TaxiTrip\n",
      "total_amount\n",
      "  Value error, Los montos deben ser positivos o cero [type=value_error, input_value=-3.25, input_type=float]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/value_error\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:42:22,618 - nyc_taxi_etl - WARNING - Error de validación en el registro 172424: 1 validation error for TaxiTrip\n",
      "total_amount\n",
      "  Value error, Los montos deben ser positivos o cero [type=value_error, input_value=-4.0, input_type=float]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/value_error\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:42:22,618 [WARNING] Error de validación en el registro 172424: 1 validation error for TaxiTrip\n",
      "total_amount\n",
      "  Value error, Los montos deben ser positivos o cero [type=value_error, input_value=-4.0, input_type=float]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/value_error\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:42:23,542 - nyc_taxi_etl - WARNING - Error de validación en el registro 217612: 1 validation error for TaxiTrip\n",
      "total_amount\n",
      "  Value error, Los montos deben ser positivos o cero [type=value_error, input_value=-4.0, input_type=float]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/value_error\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:42:23,542 [WARNING] Error de validación en el registro 217612: 1 validation error for TaxiTrip\n",
      "total_amount\n",
      "  Value error, Los montos deben ser positivos o cero [type=value_error, input_value=-4.0, input_type=float]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/value_error\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:42:23,563 - nyc_taxi_etl - WARNING - Error de validación en el registro 218647: 1 validation error for TaxiTrip\n",
      "total_amount\n",
      "  Value error, Los montos deben ser positivos o cero [type=value_error, input_value=-1.0, input_type=float]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/value_error\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:42:23,563 [WARNING] Error de validación en el registro 218647: 1 validation error for TaxiTrip\n",
      "total_amount\n",
      "  Value error, Los montos deben ser positivos o cero [type=value_error, input_value=-1.0, input_type=float]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/value_error\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:42:23,988 - nyc_taxi_etl - WARNING - Error de validación en el registro 241072: 1 validation error for TaxiTrip\n",
      "total_amount\n",
      "  Value error, Los montos deben ser positivos o cero [type=value_error, input_value=-4.0, input_type=float]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/value_error\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:42:23,988 [WARNING] Error de validación en el registro 241072: 1 validation error for TaxiTrip\n",
      "total_amount\n",
      "  Value error, Los montos deben ser positivos o cero [type=value_error, input_value=-4.0, input_type=float]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/value_error\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:42:24,212 - nyc_taxi_etl - WARNING - Error de validación en el registro 257483: 1 validation error for TaxiTrip\n",
      "total_amount\n",
      "  Value error, Los montos deben ser positivos o cero [type=value_error, input_value=-5.75, input_type=float]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/value_error\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:42:24,212 [WARNING] Error de validación en el registro 257483: 1 validation error for TaxiTrip\n",
      "total_amount\n",
      "  Value error, Los montos deben ser positivos o cero [type=value_error, input_value=-5.75, input_type=float]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/value_error\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:42:24,238 - nyc_taxi_etl - WARNING - Error de validación en el registro 258882: 1 validation error for TaxiTrip\n",
      "total_amount\n",
      "  Value error, Los montos deben ser positivos o cero [type=value_error, input_value=-4.0, input_type=float]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/value_error\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:42:24,238 [WARNING] Error de validación en el registro 258882: 1 validation error for TaxiTrip\n",
      "total_amount\n",
      "  Value error, Los montos deben ser positivos o cero [type=value_error, input_value=-4.0, input_type=float]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/value_error\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:43:06,453 - nyc_taxi_etl - INFO - Validación completada. Registros válidos: 2869996, Errores: 80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:43:06,453 [INFO] Validación completada. Registros válidos: 2869996, Errores: 80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:43:06,474 - nyc_taxi_etl - INFO - Iniciando carga de datos en la base de datos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:43:06,474 [INFO] Iniciando carga de datos en la base de datos\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:43:21,939 - nyc_taxi_etl - INFO - Lote procesado: 1 a 100000 de 2869996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:43:21,939 [INFO] Lote procesado: 1 a 100000 de 2869996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:43:34,065 - nyc_taxi_etl - INFO - Lote procesado: 100001 a 200000 de 2869996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:43:34,065 [INFO] Lote procesado: 100001 a 200000 de 2869996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:43:47,096 - nyc_taxi_etl - INFO - Lote procesado: 200001 a 300000 de 2869996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:43:47,096 [INFO] Lote procesado: 200001 a 300000 de 2869996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:43:59,933 - nyc_taxi_etl - INFO - Lote procesado: 300001 a 400000 de 2869996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:43:59,933 [INFO] Lote procesado: 300001 a 400000 de 2869996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:44:11,626 - nyc_taxi_etl - INFO - Lote procesado: 400001 a 500000 de 2869996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:44:11,626 [INFO] Lote procesado: 400001 a 500000 de 2869996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:44:23,267 - nyc_taxi_etl - INFO - Lote procesado: 500001 a 600000 de 2869996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:44:23,267 [INFO] Lote procesado: 500001 a 600000 de 2869996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:44:35,738 - nyc_taxi_etl - INFO - Lote procesado: 600001 a 700000 de 2869996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:44:35,738 [INFO] Lote procesado: 600001 a 700000 de 2869996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:44:47,857 - nyc_taxi_etl - INFO - Lote procesado: 700001 a 800000 de 2869996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:44:47,857 [INFO] Lote procesado: 700001 a 800000 de 2869996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:45:00,519 - nyc_taxi_etl - INFO - Lote procesado: 800001 a 900000 de 2869996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:45:00,519 [INFO] Lote procesado: 800001 a 900000 de 2869996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:45:12,885 - nyc_taxi_etl - INFO - Lote procesado: 900001 a 1000000 de 2869996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:45:12,885 [INFO] Lote procesado: 900001 a 1000000 de 2869996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:45:25,004 - nyc_taxi_etl - INFO - Lote procesado: 1000001 a 1100000 de 2869996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:45:25,004 [INFO] Lote procesado: 1000001 a 1100000 de 2869996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:45:37,006 - nyc_taxi_etl - INFO - Lote procesado: 1100001 a 1200000 de 2869996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:45:37,006 [INFO] Lote procesado: 1100001 a 1200000 de 2869996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:45:48,597 - nyc_taxi_etl - INFO - Lote procesado: 1200001 a 1300000 de 2869996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:45:48,597 [INFO] Lote procesado: 1200001 a 1300000 de 2869996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:46:00,872 - nyc_taxi_etl - INFO - Lote procesado: 1300001 a 1400000 de 2869996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:46:00,872 [INFO] Lote procesado: 1300001 a 1400000 de 2869996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:46:13,297 - nyc_taxi_etl - INFO - Lote procesado: 1400001 a 1500000 de 2869996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:46:13,297 [INFO] Lote procesado: 1400001 a 1500000 de 2869996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:46:25,163 - nyc_taxi_etl - INFO - Lote procesado: 1500001 a 1600000 de 2869996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:46:25,163 [INFO] Lote procesado: 1500001 a 1600000 de 2869996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:46:36,925 - nyc_taxi_etl - INFO - Lote procesado: 1600001 a 1700000 de 2869996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:46:36,925 [INFO] Lote procesado: 1600001 a 1700000 de 2869996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:46:48,580 - nyc_taxi_etl - INFO - Lote procesado: 1700001 a 1800000 de 2869996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:46:48,580 [INFO] Lote procesado: 1700001 a 1800000 de 2869996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:47:00,655 - nyc_taxi_etl - INFO - Lote procesado: 1800001 a 1900000 de 2869996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:47:00,655 [INFO] Lote procesado: 1800001 a 1900000 de 2869996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:47:12,677 - nyc_taxi_etl - INFO - Lote procesado: 1900001 a 2000000 de 2869996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:47:12,677 [INFO] Lote procesado: 1900001 a 2000000 de 2869996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:47:24,309 - nyc_taxi_etl - INFO - Lote procesado: 2000001 a 2100000 de 2869996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:47:24,309 [INFO] Lote procesado: 2000001 a 2100000 de 2869996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:47:35,936 - nyc_taxi_etl - INFO - Lote procesado: 2100001 a 2200000 de 2869996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:47:35,936 [INFO] Lote procesado: 2100001 a 2200000 de 2869996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:47:47,533 - nyc_taxi_etl - INFO - Lote procesado: 2200001 a 2300000 de 2869996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:47:47,533 [INFO] Lote procesado: 2200001 a 2300000 de 2869996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:48:00,362 - nyc_taxi_etl - INFO - Lote procesado: 2300001 a 2400000 de 2869996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:48:00,362 [INFO] Lote procesado: 2300001 a 2400000 de 2869996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:48:13,075 - nyc_taxi_etl - INFO - Lote procesado: 2400001 a 2500000 de 2869996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:48:13,075 [INFO] Lote procesado: 2400001 a 2500000 de 2869996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:48:25,313 - nyc_taxi_etl - INFO - Lote procesado: 2500001 a 2600000 de 2869996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:48:25,313 [INFO] Lote procesado: 2500001 a 2600000 de 2869996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:48:36,511 - nyc_taxi_etl - INFO - Lote procesado: 2600001 a 2700000 de 2869996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:48:36,511 [INFO] Lote procesado: 2600001 a 2700000 de 2869996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:48:47,926 - nyc_taxi_etl - INFO - Lote procesado: 2700001 a 2800000 de 2869996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:48:47,926 [INFO] Lote procesado: 2700001 a 2800000 de 2869996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:48:56,004 - nyc_taxi_etl - INFO - Lote procesado: 2800001 a 2869996 de 2869996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:48:56,004 [INFO] Lote procesado: 2800001 a 2869996 de 2869996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:48:56,008 - nyc_taxi_etl - INFO - Carga de datos completada exitosamente\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:48:56,008 [INFO] Carga de datos completada exitosamente\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:48:56,016 - nyc_taxi_etl - INFO - Flujo ETL completado exitosamente\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:48:56,016 [INFO] Flujo ETL completado exitosamente\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ETL completado en 414.95 segundos\n"
     ]
    }
   ],
   "source": [
    "# Ejecutar el flujo ETL y medir el tiempo\n",
    "start_time = time.time()\n",
    "\n",
    "# Ejecutar el flujo\n",
    "nyc_taxi_etl_flow()\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "print(f\"\\nETL completado en {execution_time:.2f} segundos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verificando los Resultados\n",
    "\n",
    "Vamos a verificar que los datos se hayan cargado correctamente en la base de datos SQLite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de viajes en la base de datos: 2869996\n",
      "Número de ubicaciones en la base de datos: 262\n",
      "\n",
      "Muestra de viajes:\n",
      "shape: (5, 18)\n",
      "┌─────┬───────────┬────────────┬────────────┬───┬────────────┬────────────┬────────────┬───────────┐\n",
      "│ id  ┆ vendor_id ┆ pickup_dat ┆ dropoff_da ┆ … ┆ total_amou ┆ congestion ┆ airport_fe ┆ payment_t │\n",
      "│ --- ┆ ---       ┆ etime      ┆ tetime     ┆   ┆ nt         ┆ _surcharge ┆ e          ┆ ype       │\n",
      "│ i64 ┆ i64       ┆ ---        ┆ ---        ┆   ┆ ---        ┆ ---        ┆ ---        ┆ ---       │\n",
      "│     ┆           ┆ str        ┆ str        ┆   ┆ f64        ┆ f64        ┆ f64        ┆ i64       │\n",
      "╞═════╪═══════════╪════════════╪════════════╪═══╪════════════╪════════════╪════════════╪═══════════╡\n",
      "│ 1   ┆ 2         ┆ 2024-01-01 ┆ 2024-01-01 ┆ … ┆ 22.7       ┆ 2.5        ┆ 0.0        ┆ 2         │\n",
      "│     ┆           ┆ 00:57:55.0 ┆ 01:17:43.0 ┆   ┆            ┆            ┆            ┆           │\n",
      "│     ┆           ┆ 00000      ┆ 00000      ┆   ┆            ┆            ┆            ┆           │\n",
      "│ 2   ┆ 1         ┆ 2024-01-01 ┆ 2024-01-01 ┆ … ┆ 18.75      ┆ 2.5        ┆ 0.0        ┆ 1         │\n",
      "│     ┆           ┆ 00:03:00.0 ┆ 00:09:36.0 ┆   ┆            ┆            ┆            ┆           │\n",
      "│     ┆           ┆ 00000      ┆ 00000      ┆   ┆            ┆            ┆            ┆           │\n",
      "│ 3   ┆ 1         ┆ 2024-01-01 ┆ 2024-01-01 ┆ … ┆ 31.3       ┆ 2.5        ┆ 0.0        ┆ 1         │\n",
      "│     ┆           ┆ 00:17:06.0 ┆ 00:35:01.0 ┆   ┆            ┆            ┆            ┆           │\n",
      "│     ┆           ┆ 00000      ┆ 00000      ┆   ┆            ┆            ┆            ┆           │\n",
      "│ 4   ┆ 1         ┆ 2024-01-01 ┆ 2024-01-01 ┆ … ┆ 17.0       ┆ 2.5        ┆ 0.0        ┆ 1         │\n",
      "│     ┆           ┆ 00:36:38.0 ┆ 00:44:56.0 ┆   ┆            ┆            ┆            ┆           │\n",
      "│     ┆           ┆ 00000      ┆ 00000      ┆   ┆            ┆            ┆            ┆           │\n",
      "│ 5   ┆ 1         ┆ 2024-01-01 ┆ 2024-01-01 ┆ … ┆ 16.1       ┆ 2.5        ┆ 0.0        ┆ 1         │\n",
      "│     ┆           ┆ 00:46:51.0 ┆ 00:52:57.0 ┆   ┆            ┆            ┆            ┆           │\n",
      "│     ┆           ┆ 00000      ┆ 00000      ┆   ┆            ┆            ┆            ┆           │\n",
      "└─────┴───────────┴────────────┴────────────┴───┴────────────┴────────────┴────────────┴───────────┘\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import polars as pl\n",
    "\n",
    "# Conectar a la base de datos\n",
    "conn = sqlite3.connect(str(DB_PATH))\n",
    "\n",
    "# Consultar usando read_database\n",
    "trip_count = pl.read_database(query=\"SELECT COUNT(*) FROM taxi_trips\", connection=conn).item(0, 0)\n",
    "location_count = pl.read_database(query=\"SELECT COUNT(*) FROM locations\", connection=conn).item(0, 0)\n",
    "\n",
    "print(f\"Número de viajes en la base de datos: {trip_count}\")\n",
    "print(f\"Número de ubicaciones en la base de datos: {location_count}\")\n",
    "\n",
    "# Consultar algunos viajes para verificar\n",
    "sample_trips = pl.read_database(query=\"SELECT * FROM taxi_trips LIMIT 5\", connection=conn)\n",
    "\n",
    "print(\"\\nMuestra de viajes:\")\n",
    "print(sample_trips)\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparación de Rendimiento: Polars vs Pandas\n",
    "\n",
    "Para demostrar las ventajas de rendimiento de Polars sobre Pandas, vamos a implementar una versión simplificada del mismo proceso ETL utilizando Pandas y comparar los tiempos de ejecución."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Benchmark: Pandas vs Polars ===\n",
      "\n",
      "1. Ejecutando ETL con Pandas...\n",
      "Extrayendo datos con Pandas...\n",
      "Extracción completada en 38.71 segundos\n",
      "Transformando datos con Pandas...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from etl_example.etl_config import TAXI_DATA_FILE\n",
    "\n",
    "def etl_with_pandas():\n",
    "    # Extracción\n",
    "    start_time = time.time()\n",
    "    print(\"Extrayendo datos con Pandas...\")\n",
    "    df_pandas = pd.read_parquet(TAXI_DATA_FILE)\n",
    "    extraction_time = time.time() - start_time\n",
    "    print(f\"Extracción completada en {extraction_time:.2f} segundos\")\n",
    "    \n",
    "    # Transformación\n",
    "    start_time = time.time()\n",
    "    print(\"Transformando datos con Pandas...\")\n",
    "    \n",
    "    # Renombrar columnas para consistencia\n",
    "    column_mapping = {\n",
    "        \"VendorID\": \"vendor_id\",\n",
    "        \"tpep_pickup_datetime\": \"pickup_datetime\",\n",
    "        \"tpep_dropoff_datetime\": \"dropoff_datetime\",\n",
    "        \"PULocationID\": \"pickup_location_id\",\n",
    "        \"DOLocationID\": \"dropoff_location_id\"\n",
    "    }\n",
    "    df_pandas = df_pandas.rename(columns=column_mapping)\n",
    "    \n",
    "    # Filtrar viajes con distancia válida\n",
    "    df_pandas = df_pandas[df_pandas['trip_distance'] > 0]\n",
    "    \n",
    "    # Filtrar viajes con tarifa válida\n",
    "    df_pandas = df_pandas[df_pandas['fare_amount'] >= 0]\n",
    "    \n",
    "    # Calcular la duración del viaje en minutos\n",
    "    df_pandas['trip_duration_minutes'] = (df_pandas['dropoff_datetime'] - df_pandas['pickup_datetime']).dt.total_seconds() / 60\n",
    "    \n",
    "    # Filtrar viajes con duración válida\n",
    "    df_pandas = df_pandas[df_pandas['trip_duration_minutes'] > 0]\n",
    "    \n",
    "    # Calcular la velocidad promedio\n",
    "    df_pandas['avg_speed_mph'] = df_pandas['trip_distance'] / (df_pandas['trip_duration_minutes'] / 60)\n",
    "    \n",
    "    # Filtrar velocidades razonables\n",
    "    df_pandas = df_pandas[df_pandas['avg_speed_mph'] < 100]\n",
    "    \n",
    "    # Manejar valores nulos\n",
    "    df_pandas['passenger_count'] = df_pandas['passenger_count'].fillna(1)\n",
    "    df_pandas['congestion_surcharge'] = df_pandas['congestion_surcharge'].fillna(0)\n",
    "    df_pandas['Airport_fee'] = df_pandas['Airport_fee'].fillna(0)\n",
    "    \n",
    "    transformation_time = time.time() - start_time\n",
    "    print(f\"Transformación completada en {transformation_time:.2f} segundos\")\n",
    "    \n",
    "    return {\n",
    "        \"extraction_time\": extraction_time,\n",
    "        \"transformation_time\": transformation_time,\n",
    "        \"total_time\": extraction_time + transformation_time,\n",
    "        \"row_count\": len(df_pandas)\n",
    "    }\n",
    "\n",
    "def etl_with_polars():\n",
    "    import polars as pl\n",
    "    \n",
    "    # Extracción\n",
    "    start_time = time.time()\n",
    "    print(\"Extrayendo datos con Polars...\")\n",
    "    df_polars = pl.read_parquet(TAXI_DATA_FILE)\n",
    "    extraction_time = time.time() - start_time\n",
    "    print(f\"Extracción completada en {extraction_time:.2f} segundos\")\n",
    "    \n",
    "    # Transformación\n",
    "    start_time = time.time()\n",
    "    print(\"Transformando datos con Polars...\")\n",
    "    \n",
    "    # Renombrar columnas para consistencia\n",
    "    column_mapping = {\n",
    "        \"VendorID\": \"vendor_id\",\n",
    "        \"tpep_pickup_datetime\": \"pickup_datetime\",\n",
    "        \"tpep_dropoff_datetime\": \"dropoff_datetime\",\n",
    "        \"PULocationID\": \"pickup_location_id\",\n",
    "        \"DOLocationID\": \"dropoff_location_id\"\n",
    "    }\n",
    "    for old_name, new_name in column_mapping.items():\n",
    "        if old_name in df_polars.columns:\n",
    "            df_polars = df_polars.rename({old_name: new_name})\n",
    "    \n",
    "    # Filtrar viajes con distancia válida\n",
    "    df_polars = df_polars.filter(pl.col(\"trip_distance\") > 0)\n",
    "    \n",
    "    # Filtrar viajes con tarifa válida\n",
    "    df_polars = df_polars.filter(pl.col(\"fare_amount\") >= 0)\n",
    "    \n",
    "    # Calcular la duración del viaje en minutos\n",
    "    df_polars = df_polars.with_columns([\n",
    "        ((pl.col(\"dropoff_datetime\").dt.epoch() - pl.col(\"pickup_datetime\").dt.epoch()) / 60).alias(\"trip_duration_minutes\")\n",
    "    ])\n",
    "    \n",
    "    # Filtrar viajes con duración válida\n",
    "    df_polars = df_polars.filter(pl.col(\"trip_duration_minutes\") > 0)\n",
    "    \n",
    "    # Calcular la velocidad promedio\n",
    "    df_polars = df_polars.with_columns([\n",
    "        (pl.col(\"trip_distance\") / (pl.col(\"trip_duration_minutes\") / 60)).alias(\"avg_speed_mph\")\n",
    "    ])\n",
    "    \n",
    "    # Filtrar velocidades razonables\n",
    "    df_polars = df_polars.filter(pl.col(\"avg_speed_mph\") < 100)\n",
    "    \n",
    "    # Manejar valores nulos\n",
    "    df_polars = df_polars.with_columns([\n",
    "        pl.col(\"passenger_count\").fill_null(1),\n",
    "        pl.col(\"congestion_surcharge\").fill_null(0),\n",
    "        pl.col(\"Airport_fee\").fill_null(0)\n",
    "    ])\n",
    "    \n",
    "    transformation_time = time.time() - start_time\n",
    "    print(f\"Transformación completada en {transformation_time:.2f} segundos\")\n",
    "    \n",
    "    return {\n",
    "        \"extraction_time\": extraction_time,\n",
    "        \"transformation_time\": transformation_time,\n",
    "        \"total_time\": extraction_time + transformation_time,\n",
    "        \"row_count\": df_polars.shape[0]\n",
    "    }\n",
    "\n",
    "# Ejecutar ambas versiones y comparar\n",
    "print(\"=== Benchmark: Pandas vs Polars ===\")\n",
    "print(\"\\n1. Ejecutando ETL con Pandas...\")\n",
    "pandas_results = etl_with_pandas()\n",
    "\n",
    "print(\"\\n2. Ejecutando ETL con Polars...\")\n",
    "polars_results = etl_with_polars()\n",
    "\n",
    "# Calcular la mejora de rendimiento\n",
    "speedup_extraction = pandas_results[\"extraction_time\"] / polars_results[\"extraction_time\"]\n",
    "speedup_transformation = pandas_results[\"transformation_time\"] / polars_results[\"transformation_time\"]\n",
    "speedup_total = pandas_results[\"total_time\"] / polars_results[\"total_time\"]\n",
    "\n",
    "print(\"\\n=== Resultados del Benchmark ===\")\n",
    "print(f\"Filas procesadas: {pandas_results['row_count']}\")\n",
    "print(\"\\nTiempos de Pandas:\")\n",
    "print(f\"  - Extracción: {pandas_results['extraction_time']:.2f} segundos\")\n",
    "print(f\"  - Transformación: {pandas_results['transformation_time']:.2f} segundos\")\n",
    "print(f\"  - Total: {pandas_results['total_time']:.2f} segundos\")\n",
    "\n",
    "print(\"\\nTiempos de Polars:\")\n",
    "print(f\"  - Extracción: {polars_results['extraction_time']:.2f} segundos\")\n",
    "print(f\"  - Transformación: {polars_results['transformation_time']:.2f} segundos\")\n",
    "print(f\"  - Total: {polars_results['total_time']:.2f} segundos\")\n",
    "\n",
    "print(\"\\nMejora de rendimiento (Polars vs Pandas):\")\n",
    "print(f\"  - Extracción: {speedup_extraction:.2f}x más rápido\")\n",
    "print(f\"  - Transformación: {speedup_transformation:.2f}x más rápido\")\n",
    "print(f\"  - Total: {speedup_total:.2f}x más rápido\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ventajas de Polars para ETL\n",
    "\n",
    "Basándonos en la implementación y los resultados del benchmark, podemos destacar las siguientes ventajas de Polars para procesos ETL:\n",
    "\n",
    "1. **Rendimiento superior**: Como hemos visto en el benchmark, Polars es significativamente más rápido que Pandas en operaciones de extracción y transformación.\n",
    "\n",
    "2. **Ejecución perezosa (lazy)**: Polars permite definir un plan de ejecución completo antes de ejecutarlo, lo que permite optimizaciones globales.\n",
    "\n",
    "3. **Paralelismo automático**: Polars aprovecha automáticamente todos los núcleos disponibles sin configuración adicional.\n",
    "\n",
    "4. **Eficiencia de memoria**: Polars consume menos memoria que Pandas para las mismas operaciones.\n",
    "\n",
    "5. **API expresiva**: La API de Polars permite expresar transformaciones complejas de manera concisa y legible.\n",
    "\n",
    "6. **Integración con ecosistema de datos**: Polars se integra bien con formatos como Parquet, CSV, JSON, etc.\n",
    "\n",
    "7. **Consistencia de API**: La API de Polars es más consistente y predecible que la de Pandas.\n",
    "\n",
    "Estas ventajas hacen de Polars una excelente opción para procesos ETL que manejan conjuntos de datos medianos a grandes en una sola máquina."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ventajas de la Arquitectura ETL Implementada\n",
    "\n",
    "Nuestra arquitectura ETL combina varias tecnologías modernas para crear un flujo de trabajo robusto y eficiente:\n",
    "\n",
    "1. **Polars para procesamiento de datos**: Aprovechamos el rendimiento y la expresividad de Polars para las operaciones de extracción y transformación.\n",
    "\n",
    "2. **Pydantic para validación de datos**: Utilizamos Pydantic para asegurar que los datos cumplen con nuestras expectativas antes de cargarlos en la base de datos.\n",
    "\n",
    "3. **SQLAlchemy para acceso a base de datos**: Utilizamos SQLAlchemy para definir el esquema de la base de datos y gestionar las conexiones de manera orientada a objetos.\n",
    "\n",
    "4. **Logging para seguimiento**: Configuramos un sistema de logging para seguir el progreso del ETL y diagnosticar problemas.\n",
    "\n",
    "Esta arquitectura proporciona:\n",
    "\n",
    "- **Modularidad**: Cada componente tiene una responsabilidad clara y puede ser modificado o reemplazado independientemente.\n",
    "- **Escalabilidad**: El diseño permite escalar a conjuntos de datos más grandes y flujos de trabajo más complejos.\n",
    "- **Mantenibilidad**: El código está organizado de manera lógica y sigue buenas prácticas de ingeniería de software.\n",
    "- **Robustez**: La validación de datos y el manejo de errores aseguran que el ETL sea resistente a problemas.\n",
    "- **Observabilidad**: El logging y la monitorización permiten seguir el progreso y diagnosticar problemas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones\n",
    "\n",
    "En este ejemplo, hemos implementado un ETL completo utilizando Polars para procesar el dataset de taxis de Nueva York. Hemos demostrado las ventajas de Polars sobre Pandas en términos de rendimiento y funcionalidades, y hemos construido una arquitectura ETL robusta y eficiente.\n",
    "\n",
    "Las principales conclusiones son:\n",
    "\n",
    "1. **Polars ofrece un rendimiento significativamente mejor que Pandas** para operaciones ETL, especialmente en conjuntos de datos medianos a grandes.\n",
    "\n",
    "2. **La combinación de Polars, Pydantic, SQLAlchemy y un Orquestador** proporciona una arquitectura ETL robusta, eficiente y mantenible.\n",
    "\n",
    "3. **La validación estricta de tipos con Pydantic** asegura la integridad de los datos antes de cargarlos en la base de datos.\n",
    "\n",
    "4. **La implementación de DAGs con Orquestador** permite definir flujos de trabajo complejos de manera clara y gestionar errores de manera efectiva.\n",
    "\n",
    "5. **El logging y la monitorización** son esenciales para seguir el progreso del ETL y diagnosticar problemas.\n",
    "\n",
    "En la siguiente sección, presentaremos un ejercicio práctico para que los estudiantes implementen su propio ETL utilizando estas tecnologías."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
